{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luca-arts/seeingtheimperceptible/blob/main/notebooks/total_flow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auv1_Ky6GZlc"
      },
      "source": [
        "# Seeing the imperceptible: total flow\n",
        "\n",
        "this is a notebook which is used to have a set of images move through the processing steps which can individually be found in the topic related folders.\n",
        "\n",
        "A **batch** of images will be processed and all intermediate steps will be saved. This is not an optimized flow, yet a flow allowing the user to intervene where necessary and have updated images continue throughout the flow.\n",
        "\n",
        "This flow is created to test with some experts and capture their feedback, it is not intended for day-to-day usage. One testing day will be organized in which this notebook will be used with some unique images.\n",
        "\n",
        "## steps\n",
        "\n",
        "1. Sensor dust removal\n",
        "2. Image Editing (LaMa)\n",
        "3. Background Removal\n",
        "4. Background Recoloring: not yet\n",
        "5. Clothes recoloring: not yet\n",
        "6. Skin retouching\n",
        "<!-- 7. Face Detection -->\n",
        "7. optional: Color Corrections: not yet\n",
        "8. Color Grading: \n",
        "9. Image upscaling\n",
        "\n",
        "TODO: is it necessary to create a config file and have separate notebooks for each step? (with 1 common config generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrpFLD9BGU3q",
        "outputId": "28124eb2-363e-487d-98bd-f47f83139bfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "what's the username for nextcloud? colab\n",
            "what's the password for user colab? ··········\n",
            "0\n",
            "Please enter the username to authenticate with server\n",
            "https://cloud.bxlab.net/remote.php/dav/files/colab/colabfiles/ or hit enter for none.\n",
            "  Username: Please enter the password to authenticate user colab with server\n",
            "https://cloud.bxlab.net/remote.php/dav/files/colab/colabfiles/ or hit enter for none.\n",
            "  Password:  \n"
          ]
        }
      ],
      "source": [
        "# first we'll link a database connection:\n",
        "!curl https://raw.githubusercontent.com/luca-arts/seeingtheimperceptible/main/notebooks/database_mod.py -o /content/database_mod.py --silent\n",
        "from database_mod import *\n",
        "\n",
        "link_nextcloud()\n",
        "\n",
        "nextcloud = '/content/database/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv-3JoWpICRe"
      },
      "source": [
        "## SETUP\n",
        "\n",
        "we'll link this instance of the machine learning flow to your name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PpvzPCmICRe",
        "outputId": "932831e2-e564-45ff-c6e0-86dd70402147"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Are you sure you don't want to change the name?\n"
          ]
        }
      ],
      "source": [
        "tname = 'total' #@param {type:\"string\"}\n",
        "if(tname=='total'):\n",
        "    print(\"Are you sure you don't want to change the name?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB5-jsXmItCP"
      },
      "source": [
        "## Step 1: Sensor dust removal\n",
        "\n",
        "we'll link the main input folder and write the output images in the output folder of step 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gvKTWPdHkHN",
        "outputId": "ed4df4e6-442b-4805-8da9-cf77a1dca883"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  1255  100  1255    0     0  25100      0 --:--:-- --:--:-- --:--:-- 25100\n"
          ]
        }
      ],
      "source": [
        "#@title imports of libraries & setting up\n",
        "input_step1, output_step1 = create_io(database=nextcloud,topic=tname,library='step1_sensor_dust', input_redirect='/content/database/total/input')\n",
        "\n",
        "#import libraries\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import os, sys\n",
        "!curl https://raw.githubusercontent.com/Tschucker/Python-Automatic-Sensor-Dust-Removal/main/shapedetector.py -o /content/shapedetector.py\n",
        "module_path = os.path.abspath(os.path.join('.'))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "from shapedetector import ShapeDetector\n",
        "import imutils\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UWZFZUVI36H",
        "outputId": "22599677-c1bd-4139-8409-9988c0c74147"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "43.jpg /content/database/total/input\n",
            "44.jpg /content/database/total/input\n",
            "45.jpg /content/database/total/input\n",
            "46.jpg /content/database/total/input\n",
            "47.jpg /content/database/total/input\n",
            "48.jpg /content/database/total/input\n",
            "49.jpg /content/database/total/input\n",
            "5.jpg /content/database/total/input\n",
            "50.jpg /content/database/total/input\n"
          ]
        }
      ],
      "source": [
        "#@title Set inpainting options and plot result\n",
        "radius = 11 #@param {type:\"slider\",min:1, max:50}\n",
        "flags = cv2.INPAINT_TELEA #@param [\"cv2.INPAINT_TELEA\",\"cv2.INPAINT_NS\"]\n",
        "\n",
        "def inpaint_img(img_path, img_name, output_path, radius=10, flags=cv2.INPAINT_TELEA):\n",
        "  #color version\n",
        "  cimg = cv2.imread(img_path)\n",
        "  #grey scale image\n",
        "  img = cv2.imread(img_path,0)\n",
        "\n",
        "  #Apply Global Threshold\n",
        "  m = np.mean(img, dtype=int)\n",
        "  global_thresh = cv2.threshold(img,int(m/1.2),255,cv2.THRESH_BINARY_INV)[1]\n",
        "\n",
        "  #Perform Adaptive Threshold\n",
        "  adaptive_thresh_img = cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY_INV,19,10)\n",
        "\n",
        "  #Image Magnification Filter Kernel\n",
        "  KERNEL = np.ones((10,10), dtype=int)*10\n",
        "\n",
        "  #Filter the thresholded images*\n",
        "  img_filt = cv2.filter2D(adaptive_thresh_img,-1,KERNEL)\n",
        "  #global_thresh = cv2.filter2D(global_thresh,-1,KERNEL)\n",
        "\n",
        "  #Apply multiple times\n",
        "  for i in range(2):\n",
        "      KERNEL_i = np.ones((int(10),int(10)), dtype=int)*10\n",
        "      img_filt = cv2.filter2D(img_filt,-1,KERNEL_i)\n",
        "\n",
        "  #Combine Thresholds\n",
        "  comb = img_filt + global_thresh\n",
        "\n",
        "  #Find and Classify Contours of Image\n",
        "  cnts = cv2.findContours(comb.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
        "  cnts = imutils.grab_contours(cnts)\n",
        "  sd = ShapeDetector()\n",
        "  cimg_copy = cimg.copy()\n",
        "  for c in cnts:\n",
        "      # compute the center of the contour, then detect the name of the\n",
        "      # shape using only the contour\n",
        "      M = cv2.moments(c)\n",
        "      if M[\"m00\"] != 0:\n",
        "          cX = int((M[\"m10\"] / M[\"m00\"]) * 1)\n",
        "          cY = int((M[\"m01\"] / M[\"m00\"]) * 1)\n",
        "          shape = sd.detect(c)\n",
        "          # multiply the contour (x, y)-coordinates by the resize ratio,\n",
        "          # then draw the contours and the name of the shape on the image\n",
        "          if len(c) < 50:\n",
        "              c = c.astype(\"float\")\n",
        "              c *= 1\n",
        "              c = c.astype(\"int\")\n",
        "              cv2.drawContours(cimg_copy, [c], -1, (0, 255, 0), 2)\n",
        "              cv2.putText(cimg_copy, shape, (cX, cY), cv2.FONT_HERSHEY_SIMPLEX,0.5, (255, 255, 255), 2)\n",
        "  \n",
        "  #Create Dust Mask\n",
        "  img_mask = np.zeros((img.shape[0], img.shape[1]), dtype='uint8')\n",
        "  for c in cnts:\n",
        "      # compute the center of the contour, then detect the name of the\n",
        "      # shape using only the contour\n",
        "      M = cv2.moments(c)\n",
        "      if M[\"m00\"] != 0:\n",
        "          cX = int((M[\"m10\"] / M[\"m00\"]) * 1)\n",
        "          cY = int((M[\"m01\"] / M[\"m00\"]) * 1)\n",
        "          shape = sd.detect(c)\n",
        "          # multiply the contour (x, y)-coordinates by the resize ratio,\n",
        "          # then draw the contours and the name of the shape on the image\n",
        "          if len(c) < 50:\n",
        "              c = c.astype(\"float\")\n",
        "              c *= 1\n",
        "              c = c.astype(\"int\")\n",
        "              cv2.fillPoly(img_mask, pts=[c], color=(255,255,255))\n",
        "\n",
        "    \n",
        "  #Inpaint the image\n",
        "  cimg_inpaint = cv2.inpaint(cimg, img_mask, radius, flags=flags)\n",
        "\n",
        "  #Show and Save Final Image\n",
        "  save_img_pth = os.path.join(output_path,img_name)\n",
        "  cv2.imwrite(save_img_pth, cimg_inpaint)\n",
        "\n",
        "  # plt_out = cv2.cvtColor(cimg_inpaint, cv2.COLOR_BGR2RGB)\n",
        "  # return plt_out\n",
        "\n",
        "for img_name in os.listdir(input_step1):\n",
        "  print(\"processing \",img_name)\n",
        "  img_path = os.path.join(input_step1,img_name)\n",
        "  inpaint_img(img_path, img_name, output_step1, radius=radius, flags=flags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfTsfHmAI6R4"
      },
      "source": [
        "### verification\n",
        "\n",
        "Now it's time to go to the database and verify the results. If needed we adapt the images locally. \n",
        "\n",
        "TODO: how easily can we **sync** the images to allow the experts to intervene? Is it possible with nextcloud?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0UVWDfIJmUS"
      },
      "source": [
        "## step 2: Minor retouching: image editing LaMa \n",
        "Once verified we want to continue with the next step, being image editing (LaMa model)\n",
        "\n",
        "therefore we link the output folder of previous step to the inputfolder of this step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgwqUu8IJQm_"
      },
      "outputs": [],
      "source": [
        "input_step2, output_step2 = create_io(database=nextcloud,topic=tname,library='step2_lama', input_redirect=output_step1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRUbNHttJb3l",
        "outputId": "4d6b6d8d-0728-40d1-e644-dc605d0ee92a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/database/total/step1_sensor_dust /content/database/total/step2_<...>\n"
          ]
        }
      ],
      "source": [
        "#@title imports of libraries & setting up\n",
        "root_path2 = '/content/lama'\n",
        "\n",
        "# clone the repository\n",
        "if not os.path.exists(root_path2):\n",
        "  !git clone https://github.com/saic-mdal/lama {root_path2}\n",
        "# Set up the environment\n",
        "print('\\n> Install dependencies')\n",
        "!pip install -r lama/requirements.txt --quiet\n",
        "# do we need the wget?\n",
        "!pip install wget --quiet\n",
        "\n",
        "# download the model\n",
        "print('\\n> Download the model')\n",
        "!curl -L $(yadisk-direct https://disk.yandex.ru/d/ouP6l8VJ0HpMZg) -o {root_path2}/big-lama.zip\n",
        "# todo check where the model is unzipped\n",
        "!unzip {root_path2}/big-lama.zip\n",
        "\n",
        "# fixing openCV\n",
        "print('>fixing opencv')\n",
        "!pip uninstall opencv-python-headless -y --quiet\n",
        "!pip install opencv-python-headless==4.1.2.30 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iG3cZFNfL-NZ"
      },
      "outputs": [],
      "source": [
        "import base64, os\n",
        "from IPython.display import HTML, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import wget\n",
        "from shutil import copyfile\n",
        "import shutil\n",
        "\n",
        "#@ title: imports & helper functions\n",
        "\n",
        "canvas_html = \"\"\"\n",
        "<style>\n",
        ".button {\n",
        "  background-color: #4CAF50;\n",
        "  border: none;\n",
        "  color: white;\n",
        "  padding: 15px 32px;\n",
        "  text-align: center;\n",
        "  text-decoration: none;\n",
        "  display: inline-block;\n",
        "  font-size: 16px;\n",
        "  margin: 4px 2px;\n",
        "  cursor: pointer;\n",
        "}\n",
        "</style>\n",
        "<canvas1 width=%d height=%d>\n",
        "</canvas1>\n",
        "<canvas width=%d height=%d>\n",
        "</canvas>\n",
        "\n",
        "<button class=\"button\">Finish</button>\n",
        "<script>\n",
        "var canvas = document.querySelector('canvas')\n",
        "var ctx = canvas.getContext('2d')\n",
        "\n",
        "var canvas1 = document.querySelector('canvas1')\n",
        "var ctx1 = canvas.getContext('2d')\n",
        "\n",
        "\n",
        "ctx.strokeStyle = 'red';\n",
        "\n",
        "var img = new Image();\n",
        "img.src = \"data:image/%s;charset=utf-8;base64,%s\";\n",
        "console.log(img)\n",
        "img.onload = function() {\n",
        "  ctx1.drawImage(img, 0, 0);\n",
        "};\n",
        "img.crossOrigin = 'Anonymous';\n",
        "\n",
        "ctx.clearRect(0, 0, canvas.width, canvas.height);\n",
        "\n",
        "ctx.lineWidth = %d\n",
        "var button = document.querySelector('button')\n",
        "var mouse = {x: 0, y: 0}\n",
        "\n",
        "canvas.addEventListener('mousemove', function(e) {\n",
        "  mouse.x = e.pageX - this.offsetLeft\n",
        "  mouse.y = e.pageY - this.offsetTop\n",
        "})\n",
        "canvas.onmousedown = ()=>{\n",
        "  ctx.beginPath()\n",
        "  ctx.moveTo(mouse.x, mouse.y)\n",
        "  canvas.addEventListener('mousemove', onPaint)\n",
        "}\n",
        "canvas.onmouseup = ()=>{\n",
        "  canvas.removeEventListener('mousemove', onPaint)\n",
        "}\n",
        "var onPaint = ()=>{\n",
        "  ctx.lineTo(mouse.x, mouse.y)\n",
        "  ctx.stroke()\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "  button.onclick = ()=>{\n",
        "    resolve(canvas.toDataURL('image/png'))\n",
        "  }\n",
        "})\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def draw(imgm, filename='drawing.png', w=400, h=200, line_width=1):\n",
        "  display(HTML(canvas_html % (w, h, w,h, filename.split('.')[-1], imgm, line_width)))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in os.listdir(input_step2):\n",
        "  fname = os.path.join(input_step2,i)\n",
        "\n",
        "  image64 = base64.b64encode(open(fname, 'rb').read())\n",
        "  image64 = image64.decode('utf-8')\n",
        "\n",
        "  print(f'Will use {fname} for inpainting')\n",
        "  img = np.array(plt.imread(f'{fname}')[:,:,:3])\n",
        "\n",
        "  draw(image64, filename=f\"./{fname.split('.')[1]}_mask.png\", w=img.shape[1], h=img.shape[0], line_width=0.04*img.shape[1])\n",
        "  #@title Show a masked image and save a mask\n",
        "\n",
        "  # plt.rcParams[\"figure.figsize\"] = (15,5)\n",
        "  # plt.rcParams['figure.dpi'] = 200\n",
        "  # plt.subplot(131)\n",
        "  # with_mask = np.array(plt.imread(f\"./{fname.split('.')[1]}_mask.png\")[:,:,:3])\n",
        "  # mask = (with_mask[:,:,0]==1)*(with_mask[:,:,1]==0)*(with_mask[:,:,2]==0)\n",
        "  # plt.imshow(mask, cmap='gray')\n",
        "  # plt.axis('off')\n",
        "  # plt.title('mask')\n",
        "  # plt.imsave(f\"./{fname.split('.')[1]}_mask.png\",mask, cmap='gray')\n",
        "\n",
        "  # plt.subplot(132)\n",
        "  # img = np.array(plt.imread(f'{fname}')[:,:,:3])\n",
        "  # plt.imshow(img)\n",
        "  # plt.axis('off')\n",
        "  # plt.title('img')\n",
        "\n",
        "  # plt.subplot(133)\n",
        "  # img = np.array((1-mask.reshape(mask.shape[0], mask.shape[1], -1))*plt.imread(fname)[:,:,:3])\n",
        "  # _=plt.imshow(img)\n",
        "  # _=plt.axis('off')\n",
        "  # _=plt.title('img * mask')\n",
        "  # plt.show()\n",
        "\n",
        "  # os.makedirs('/content/output')\n",
        "\n",
        "  print('Run inpainting')\n",
        "  if '.jpeg' in fname:\n",
        "    !PYTHONPATH=. TORCH_HOME=$(pwd) python3 bin/predict.py model.path=$(pwd)/big-lama indir=$(pwd)/data_for_prediction outdir={output_step2} dataset.img_suffix=.jpeg > /dev/null\n",
        "  elif '.jpg' in fname:\n",
        "    !PYTHONPATH=. TORCH_HOME=$(pwd) python3 bin/predict.py model.path=$(pwd)/big-lama indir=$(pwd)/data_for_prediction outdir={output_step2}  dataset.img_suffix=.jpg   > /dev/null\n",
        "  elif '.png' in fname:\n",
        "    !PYTHONPATH=. TORCH_HOME=$(pwd) python3 bin/predict.py model.path=$(pwd)/big-lama indir=$(pwd)/data_for_prediction outdir={output_step2}  dataset.img_suffix=.png > /dev/null\n",
        "  else:\n",
        "    print(f'Error: unknown suffix .{fname.split(\".\")[-1]} use [.png, .jpeg, .jpg]')\n",
        "\n",
        "  plt.rcParams['figure.dpi'] = 200\n",
        "\n",
        "  print(os.listdir(output_step2))\n",
        "\n",
        "# plt.imshow(plt.imread(f\"/content/output/{fname.split('.')[1].split('/')[2]}_mask.png\"))\n",
        "# _=plt.axis('off')\n",
        "# _=plt.title('inpainting result')\n",
        "# plt.show()\n",
        "\n",
        "# fname = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVknZ2QiAzSE"
      },
      "source": [
        "## Step 3: background removal\n",
        "\n",
        "Again, verify the outcome of step 2. \n",
        "\n",
        "Now we'll subtract the background using the ModNet model.\n",
        "\n",
        "<!-- Or use the other model if better -->\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAAAIFzIBD_Z"
      },
      "outputs": [],
      "source": [
        "input_step3, output_step3 = create_io(database=nextcloud,topic=tname,library='step3_bg_removal', input_redirect=output_step2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2dRnK6VBMlj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpU9yCF0BLkF"
      },
      "source": [
        "## Step 4: Background recoloring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6e2Sl08BZQs"
      },
      "outputs": [],
      "source": [
        "input_step4, output_step4 = create_io(database=nextcloud,topic=tname,library='step4_bg_coloring', input_redirect=output_step3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEm4TRT1BfEt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHvEogQSBfUh"
      },
      "source": [
        "## Step 5: Clothes recoloring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFRDNlwvICRl"
      },
      "outputs": [],
      "source": [
        "input_step5, output_step5 = create_io(database=nextcloud,topic=tname,library='step5_clothes_coloring', input_redirect=output_step4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ypRPB6yICRl"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "# as we're not implementing clothes recoloring yet, we copy the folders to surpass this step\n",
        "shutil.copytree(output_step4,output_step5, dirs_exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jIiSGm6ICRl"
      },
      "source": [
        "## Step 6: Skin retouching\n",
        "\n",
        "we'll implement the retouchML library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2tQDXn-ICRm"
      },
      "outputs": [],
      "source": [
        "input_step6, output_step6 = create_io(database=nextcloud,topic=tname,library='step6_skin_retouch', input_redirect=output_step5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddpvlvl6ICRm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ceij8_mpICRm"
      },
      "source": [
        "## Step 7: Color Corrections\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nb06tfujICRm"
      },
      "outputs": [],
      "source": [
        "input_step7, output_step7 = create_io(database=nextcloud,topic=tname,library='step6_color_corrections', input_redirect=output_step6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_epn_TzNICRn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gHgBX7oICRn"
      },
      "source": [
        "## Step 8: Color Grading\n",
        "\n",
        "we're implementing the deep preset library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDoSTIKlICRn"
      },
      "outputs": [],
      "source": [
        "input_step8, output_step8 = create_io(database=nextcloud,topic=tname,library='step7_color_grading', input_redirect=output_step7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3_e2_XYICRn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCYexnqCICRn"
      },
      "source": [
        "## Step 9: image upscaling\n",
        "\n",
        "This is to be implemented **if** necessary?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmzznyJ8ICRo"
      },
      "outputs": [],
      "source": [
        "input_step9, output_step9 = create_io(database=nextcloud,topic=tname,library='step8_image_upscaling', input_redirect=output_step8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQsy0whPICRo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Untitled12.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
