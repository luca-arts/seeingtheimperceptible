{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luca-arts/seeingtheimperceptible/blob/main/notebooks/syntheticPoseImageGeneration/tests/Img2Txt_interrogator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytxkysgmrJEi"
      },
      "source": [
        "## Image To Text Prompt CLIP Interrogator\n",
        "by [@pharmapsychotic](https://twitter.com/pharmapsychotic) \n",
        "\n",
        "<br>\n",
        "\n",
        "What do the different OpenAI CLIP models see in an image? What might be a good text prompt to create similar images using CLIP guided diffusion or another text to image model? The CLIP Interrogator is here to get you answers!\n",
        "\n",
        "<br>\n",
        "And if you're looking for more Ai art tools check out my [Ai generative art tools list](https://pharmapsychotic.com/tools.html).\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Preparations\n",
        "\n",
        "Before start, make sure that you choose\n",
        "\n",
        "Runtime Type = Python 3\n",
        "Hardware Accelerator = GPU"
      ],
      "metadata": {
        "id": "F5-OGKI0ggpo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQk0eemUrSC7"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. linking nextcloud\n",
        "\n",
        "Connecting to the external NextCloud drive "
      ],
      "metadata": {
        "id": "ZsiNZqqhgpM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we'll link the dataset from next-cloud\n",
        "!curl https://raw.githubusercontent.com/luca-arts/seeingtheimperceptible/main/notebooks/database_mod.py -o /content/database_mod.py\n",
        "\n",
        "from database_mod import *\n",
        "\n",
        "link_nextcloud()\n",
        "\n",
        "nextcloud = '/content/database/'\n",
        "\n",
        "input_folder, output_folder = create_io(database=nextcloud,topic='syntheticPoseImageGeneration/Img2TXT',library='Img2TXT')"
      ],
      "metadata": {
        "id": "P-LN0BdEgufd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Setting up the environment"
      ],
      "metadata": {
        "id": "MB014BS4hBQ3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30xPxDSDrJEl"
      },
      "outputs": [],
      "source": [
        "!pip3 install ftfy regex tqdm transformers==4.15.0 timm==0.4.12 fairscale==0.4.4\n",
        "!pip3 install git+https://github.com/openai/CLIP.git\n",
        "!git clone https://github.com/pharmapsychotic/clip-interrogator.git\n",
        "!git clone https://github.com/salesforce/BLIP\n",
        "%cd /content/BLIP\n",
        "\n",
        "import clip\n",
        "import gc\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "from IPython.display import display\n",
        "from PIL import Image\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from models.blip import blip_decoder\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "blip_image_eval_size = 384\n",
        "blip_model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model*_base_caption.pth'        \n",
        "blip_model = blip_decoder(pretrained=blip_model_url, image_size=blip_image_eval_size, vit='base')\n",
        "blip_model.eval()\n",
        "blip_model = blip_model.to(device)\n",
        "\n",
        "def generate_caption(pil_image):\n",
        "    gpu_image = transforms.Compose([\n",
        "        transforms.Resize((blip_image_eval_size, blip_image_eval_size), interpolation=InterpolationMode.BICUBIC),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "    ])(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        caption = blip_model.generate(gpu_image, sample=False, num_beams=3, max_length=20, min_length=5)\n",
        "    return caption[0]\n",
        "\n",
        "def load_list(filename):\n",
        "    with open(filename, 'r', encoding='utf-8', errors='replace') as f:\n",
        "        items = [line.strip() for line in f.readlines()]\n",
        "    return items\n",
        "\n",
        "def rank(model, image_features, text_array, top_count=1):\n",
        "    top_count = min(top_count, len(text_array))\n",
        "    text_tokens = clip.tokenize([text for text in text_array]).cuda()\n",
        "    with torch.no_grad():\n",
        "        text_features = model.encode_text(text_tokens).float()\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    similarity = torch.zeros((1, len(text_array))).to(device)\n",
        "    for i in range(image_features.shape[0]):\n",
        "        similarity += (100.0 * image_features[i].unsqueeze(0) @ text_features.T).softmax(dim=-1)\n",
        "    similarity /= image_features.shape[0]\n",
        "\n",
        "    top_probs, top_labels = similarity.cpu().topk(top_count, dim=-1)  \n",
        "    return [(text_array[top_labels[0][i].numpy()], (top_probs[0][i].numpy()*100)) for i in range(top_count)]\n",
        "\n",
        "def interrogate(image, models):\n",
        "    caption = generate_caption(image)\n",
        "    if len(models) == 0:\n",
        "        print(f\"\\n\\n{caption}\")\n",
        "        return\n",
        "\n",
        "    table = []\n",
        "    bests = [[('',0)]]*5\n",
        "    for model_name in models:\n",
        "        print(f\"Interrogating with {model_name}...\")\n",
        "        model, preprocess = clip.load(model_name)\n",
        "        model.cuda().eval()\n",
        "\n",
        "        images = preprocess(image).unsqueeze(0).cuda()\n",
        "        with torch.no_grad():\n",
        "            image_features = model.encode_image(images).float()\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        ranks = [\n",
        "            rank(model, image_features, mediums),\n",
        "            rank(model, image_features, [\"by \"+artist for artist in artists]),\n",
        "            rank(model, image_features, trending_list),\n",
        "            rank(model, image_features, movements),\n",
        "            rank(model, image_features, flavors, top_count=3)\n",
        "        ]\n",
        "\n",
        "        for i in range(len(ranks)):\n",
        "            confidence_sum = 0\n",
        "            for ci in range(len(ranks[i])):\n",
        "                confidence_sum += ranks[i][ci][1]\n",
        "            if confidence_sum > sum(bests[i][t][1] for t in range(len(bests[i]))):\n",
        "                bests[i] = ranks[i]\n",
        "\n",
        "        row = [model_name]\n",
        "        for r in ranks:\n",
        "            row.append(', '.join([f\"{x[0]} ({x[1]:0.1f}%)\" for x in r]))\n",
        "\n",
        "        table.append(row)\n",
        "\n",
        "        del model\n",
        "        gc.collect()\n",
        "    display(pd.DataFrame(table, columns=[\"Model\", \"Medium\", \"Artist\", \"Trending\", \"Movement\", \"Flavors\"]))\n",
        "\n",
        "    flaves = ', '.join([f\"{x[0]}\" for x in bests[4]])\n",
        "    medium = bests[0][0][0]\n",
        "    if caption.startswith(medium):\n",
        "        print(f\"\\n\\n{caption} {bests[1][0][0]}, {bests[2][0][0]}, {bests[3][0][0]}, {flaves}\")\n",
        "    else:\n",
        "        print(f\"\\n\\n{caption}, {medium} {bests[1][0][0]}, {bests[2][0][0]}, {bests[3][0][0]}, {flaves}\")\n",
        "\n",
        "data_path = \"../clip-interrogator/data/\"\n",
        "\n",
        "artists = load_list(os.path.join(data_path, 'artists.txt'))\n",
        "flavors = load_list(os.path.join(data_path, 'flavors.txt'))\n",
        "mediums = load_list(os.path.join(data_path, 'mediums.txt'))\n",
        "movements = load_list(os.path.join(data_path, 'movements.txt'))\n",
        "\n",
        "sites = ['Artstation', 'behance', 'cg society', 'cgsociety', 'deviantart', 'dribble', 'flickr', 'instagram', 'pexels', 'pinterest', 'pixabay', 'pixiv', 'polycount', 'reddit', 'shutterstock', 'tumblr', 'unsplash', 'zbrush central']\n",
        "trending_list = [site for site in sites]\n",
        "trending_list.extend([\"trending on \"+site for site in sites])\n",
        "trending_list.extend([\"featured on \"+site for site in sites])\n",
        "trending_list.extend([site+\" contest winner\" for site in sites])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. IMG2TXT : Interrogator"
      ],
      "metadata": {
        "id": "jtm7CUnohULf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbDEMDGJrJEo"
      },
      "outputs": [],
      "source": [
        "#@markdown #####**Image:**\n",
        "\n",
        "image_path_or_url = \"https://cdnb.artstation.com/p/assets/images/images/032/142/769/large/ignacio-bazan-lazcano-book-4-final.jpg\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown \n",
        "\n",
        "#@markdown #####**CLIP models:**\n",
        "\n",
        "#@markdown For [StableDiffusion](https://stability.ai/blog/stable-diffusion-announcement) you can just use ViTL14<br>\n",
        "#@markdown For [DiscoDiffusion](https://colab.research.google.com/github/alembics/disco-diffusion/blob/main/Disco_Diffusion.ipynb) and \n",
        "#@markdown [JAX](https://colab.research.google.com/github/huemin-art/jax-guided-diffusion/blob/v2.7/Huemin_Jax_Diffusion_2_7.ipynb) enable all the same models here as you intend to use when generating your images\n",
        "\n",
        "ViTB32 = True #@param{type:\"boolean\"}\n",
        "ViTB16 = True #@param{type:\"boolean\"}\n",
        "ViTL14 = False #@param{type:\"boolean\"}\n",
        "ViTL14_336px = False #@param{type:\"boolean\"}\n",
        "RN101 = False #@param{type:\"boolean\"}\n",
        "RN50 = True #@param{type:\"boolean\"}\n",
        "RN50x4 = False #@param{type:\"boolean\"}\n",
        "RN50x16 = False #@param{type:\"boolean\"}\n",
        "RN50x64 = False #@param{type:\"boolean\"}\n",
        "\n",
        "models = []\n",
        "if ViTB32: models.append('ViT-B/32')\n",
        "if ViTB16: models.append('ViT-B/16')\n",
        "if ViTL14: models.append('ViT-L/14')\n",
        "if ViTL14_336px: models.append('ViT-L/14@336px')\n",
        "if RN101: models.append('RN101')\n",
        "if RN50: models.append('RN50')\n",
        "if RN50x4: models.append('RN50x4')\n",
        "if RN50x16: models.append('RN50x16')\n",
        "if RN50x64: models.append('RN50x64')\n",
        "\n",
        "if str(image_path_or_url).startswith('http://') or str(image_path_or_url).startswith('https://'):\n",
        "    image = Image.open(requests.get(image_path_or_url, stream=True).raw).convert('RGB')\n",
        "else:\n",
        "    image = Image.open(image_path_or_url).convert('RGB')\n",
        "\n",
        "thumb = image.copy()\n",
        "thumb.thumbnail([blip_image_eval_size, blip_image_eval_size])\n",
        "display(thumb)\n",
        "\n",
        "interrogate(image, models=models)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Clip-interrogator-Img2Txt.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.5 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "2e35b1f3b2666f0e402b0693dd7493a583002c98361385482aa9f27d8f0f5c89"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}