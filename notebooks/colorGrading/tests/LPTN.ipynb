{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luca-arts/seeingtheimperceptible/blob/main/notebooks/colorGrading/tests/LPTN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Color grading via LPTN\n",
        "\n",
        "[https://github.com/csjliang/LPTN](https://github.com/csjliang/LPTN)\n",
        "\n",
        "Basically they adapt the low-frequency and high-frequency components of images.\n",
        "\n",
        "In order to train this on our own data we need a dataset with :\n",
        "\n",
        "- LQ: low quality frames: images which haven't been color graded yet.\n",
        "- GT: ground truth images: images which have been color graded.\n",
        "\n",
        "I **think** right now we need to have these datasets *per* color grading theme we want to implement."
      ],
      "metadata": {
        "id": "Oka582cduBH-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1FCPYo3Wt6IL",
        "outputId": "e2c3e755-fffb-4a40-fd65-800380511c44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  2235  100  2235    0     0  12213      0 --:--:-- --:--:-- --:--:-- 12280\n",
            "what's the username for nextcloud? colab\n",
            "what's the password for user colab? ··········\n",
            "0\n",
            "Please enter the username to authenticate with server\n",
            "https://cloud.bxlab.net/remote.php/dav/files/colab/colabfiles/ or hit enter for none.\n",
            "  Username: Please enter the password to authenticate user colab with server\n",
            "https://cloud.bxlab.net/remote.php/dav/files/colab/colabfiles/ or hit enter for none.\n",
            "  Password:  \n",
            "content of /etc/fstab: https://cloud.bxlab.net/remote.php/dav/files/colab/colabfiles/ /content/database davfs user,rw,auto 0 0\n"
          ]
        }
      ],
      "source": [
        "# we'll link the dataset from next-cloud\n",
        "!curl https://raw.githubusercontent.com/luca-arts/seeingtheimperceptible/main/notebooks/database_mod.py -o /content/database_mod.py\n",
        "\n",
        "from database_mod import *\n",
        "\n",
        "link_nextcloud()\n",
        "\n",
        "nextcloud = '/content/database/'\n",
        "\n",
        "input_folder, output_folder = create_io(database=nextcloud,topic='colorGrading',library='LPTN')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Clone the Git repository"
      ],
      "metadata": {
        "id": "HiiAOBc-vTPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "root_path = '/content/LPTN'\n",
        "# clone the repository\n",
        "if not os.path.exists('LPTN'):\n",
        "  !git clone https://github.com/csjliang/LPTN {root_path}"
      ],
      "metadata": {
        "id": "bbAS06muwjiD",
        "outputId": "db4c494c-0b75-46fa-cffb-7341234cec36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/LPTN'...\n",
            "remote: Enumerating objects: 107, done.\u001b[K\n",
            "remote: Counting objects: 100% (107/107), done.\u001b[K\n",
            "remote: Compressing objects: 100% (89/89), done.\u001b[K\n",
            "remote: Total 107 (delta 26), reused 67 (delta 6), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (107/107), 263.69 KiB | 10.99 MiB/s, done.\n",
            "Resolving deltas: 100% (26/26), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. install dependencies"
      ],
      "metadata": {
        "id": "zJsUSddIw42B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qr {root_path}/requirement.txt"
      ],
      "metadata": {
        "id": "Qn5AaWfHw5af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a08d4e3-7f36-4ff3-f9c5-d8da54168439"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 5.8 MB 9.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 190 kB 43.9 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Pretrained model download"
      ],
      "metadata": {
        "id": "WIu3lf_uyVXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check first if exist, otherwise create path\n",
        "pretrained_pth = root_path + '/experiments/pretrained_models/net_g_FiveK_numhigh3.pth'\n",
        "\n",
        "if not os.path.exists(pretrained_pth):\n",
        "  !gdown --id 11yuFgHqZe9e4OheJ9YHp9lS-4kQC1Fml \\\n",
        "          -O {pretrained_pth}"
      ],
      "metadata": {
        "id": "S1VOs015yVoT",
        "outputId": "cdfa4458-f060-4287-b4cf-0ca2e0ca6348",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=11yuFgHqZe9e4OheJ9YHp9lS-4kQC1Fml\n",
            "To: /content/LPTN/experiments/pretrained_models/net_g_FiveK_numhigh3.pth\n",
            "\r  0% 0.00/2.48M [00:00<?, ?B/s]\r100% 2.48M/2.48M [00:00<00:00, 137MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. install pyYAML and update the YAML file with correct path for pretrained_models\n",
        "\n",
        "geen idee of we dit nodig hebben, maar heb zelf deze code erin gezet om te checken of het path effectief naar de goeie folder wijst.\n"
      ],
      "metadata": {
        "id": "MSFTceQ89GUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyYAML\n",
        "\n",
        "import yaml\n",
        "\n",
        "# dowload the pre-trained model for colorgrading\n",
        " \n",
        "def read_yaml(LPTN_path):\n",
        "    \"\"\" A function to read YAML file\"\"\"\n",
        "    with open('{}/options/test/LPTN/test_FiveK.yml'.format(LPTN_path)) as f:\n",
        "        config = yaml.safe_load(f)\n",
        " \n",
        "    return config\n",
        "\n",
        "read_yaml(root_path)"
      ],
      "metadata": {
        "id": "_EtdxUxI9axB",
        "outputId": "8eeda452-2b7c-44a8-e1f5-8fe4b81959b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyYAML in /usr/local/lib/python3.7/dist-packages (3.13)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'datasets': {'test': {'dataroot_gt': 'datasets/FiveK/FiveK_test_target.lmdb',\n",
              "   'dataroot_lq': 'datasets/FiveK/FiveK_test_source.lmdb',\n",
              "   'io_backend': {'type': 'lmdb'},\n",
              "   'name': 'FiveK_val',\n",
              "   'type': 'PairedImageDataset'}},\n",
              " 'manual_seed': 10,\n",
              " 'model_type': 'LPTNTestModel',\n",
              " 'name': 'LPTN_FiveK_480p',\n",
              " 'network_g': {'nrb_high': 3, 'nrb_low': 5, 'num_high': 3, 'type': 'LPTN'},\n",
              " 'num_gpu': 1,\n",
              " 'path': {'pretrain_network_g': 'experiments/pretrained_models/net_g_FiveK_numhigh3.pth',\n",
              "  'strict_load_g': False},\n",
              " 'val': {'metrics': {'psnr': {'crop_border': 0,\n",
              "    'test_y_channel': False,\n",
              "    'type': 'calculate_psnr'}},\n",
              "  'save_img': True,\n",
              "  'suffix': None}}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. dataset downloaden"
      ],
      "metadata": {
        "id": "XayoV4kjO6Nd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "download dataset"
      ],
      "metadata": {
        "id": "dvehf5N0RVdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "# define folders\n",
        "output_path_dataset = root_path +'/datasets'\n",
        "output_path_FiveK = output_path_dataset +'/FiveK'\n",
        "input_path = nextcloud + 'colorGrading/datasets/FiveK_480p.zip'\n",
        "\n",
        "# make folders\n",
        "os.makedirs(output_path_dataset, exist_ok=True)\n",
        "os.makedirs(output_path_FiveK, exist_ok=True)\n",
        "\n",
        "#!unzip \"/content/drive/path/input_file_name.zip\" -d \"/content/drive/path/output_folder/\"\n",
        "!unzip -q {input_path} -d {output_path_FiveK}\n",
        "\n",
        "%ls"
      ],
      "metadata": {
        "id": "VUSVuplwO7FN",
        "outputId": "78c3a55e-50d5-44b3-e19b-b57651d0b169",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mdatabase\u001b[0m/  database_mod.py  \u001b[01;34mLPTN\u001b[0m/  \u001b[01;34m__pycache__\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "create lmdb"
      ],
      "metadata": {
        "id": "_aDE673VRSR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/LPTN/\n",
        "!PYTHONPATH=\"/content/LPTN/:${PYTHONPATH}\" python scripts/data_preparation/create_lmdb.py"
      ],
      "metadata": {
        "id": "3dc2IO5yRREO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea712dbb-6c72-4e17-84eb-dbedf88e1686"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LPTN\n",
            "Reading image path list ...\n",
            "Create lmdb for datasets/FiveK/FiveK_480p/train/A, save to datasets/FiveK/FiveK_train_source.lmdb...\n",
            "Totoal images: 2250\n",
            "Data size per image is:  482882\n",
            "Write 998: 100% 2250/2250 [01:37<00:00, 23.03chunk/s]\n",
            "\n",
            "Finish writing lmdb.\n",
            "Reading image path list ...\n",
            "Create lmdb for datasets/FiveK/FiveK_480p/train/B, save to datasets/FiveK/FiveK_train_target.lmdb...\n",
            "Totoal images: 2250\n",
            "Data size per image is:  678345\n",
            "Write 999: 100% 2250/2250 [01:49<00:00, 20.50chunk/s]\n",
            "\n",
            "Finish writing lmdb.\n",
            "Reading image path list ...\n",
            "Create lmdb for datasets/FiveK/FiveK_480p/test/A, save to datasets/FiveK/FiveK_test_source.lmdb...\n",
            "Totoal images: 500\n",
            "Data size per image is:  308466\n",
            "Write 5000: 100% 500/500 [00:21<00:00, 23.28chunk/s]\n",
            "\n",
            "Finish writing lmdb.\n",
            "Reading image path list ...\n",
            "Create lmdb for datasets/FiveK/FiveK_480p/test/B, save to datasets/FiveK/FiveK_test_target.lmdb...\n",
            "Totoal images: 500\n",
            "Data size per image is:  611200\n",
            "Write 5000: 100% 500/500 [00:24<00:00, 20.82chunk/s]\n",
            "\n",
            "Finish writing lmdb.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Evaluation"
      ],
      "metadata": {
        "id": "hYOqeKOqEcep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "yml file test"
      ],
      "metadata": {
        "id": "41lvseFFTiMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install basicsr\n",
        "!PYTHONPATH=\"/content/LPTN/:${PYTHONPATH}\" CUDA_VISIBLE_DEVICES=0 python codes/test.py -opt options/test/LPTN/test_FiveK.yml"
      ],
      "metadata": {
        "id": "duiptbuiEgCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "337e9248-959f-4617-e2fb-fd7b2e65102d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"codes/test.py\", line 6, in <module>\n",
            "    from codes.models import create_model\n",
            "  File \"/content/LPTN/codes/models/__init__.py\", line 17, in <module>\n",
            "    for file_name in model_filenames\n",
            "  File \"/content/LPTN/codes/models/__init__.py\", line 17, in <listcomp>\n",
            "    for file_name in model_filenames\n",
            "  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/content/LPTN/codes/models/lptn_model.py\", line 16, in <module>\n",
            "    metric_module = importlib.import_module('codes.metrics')\n",
            "  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/content/LPTN/codes/metrics/__init__.py\", line 1, in <module>\n",
            "    from .psnr_ssim import calculate_psnr, calculate_ssim\n",
            "  File \"/content/LPTN/codes/metrics/psnr_ssim.py\", line 4, in <module>\n",
            "    from basicsr.metrics.metric_util import reorder_image, to_y_channel\n",
            "ModuleNotFoundError: No module named 'basicsr'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "speed testing"
      ],
      "metadata": {
        "id": "s_5Ld8TRTeZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!PYTHONPATH=\"/content/LPTN/:${PYTHONPATH}\" CUDA_VISIBLE_DEVICES=0 python codes/test_speed.py -opt options/test/LPTN/test_speed_FiveK.yml"
      ],
      "metadata": {
        "id": "0zzLtvOiTdgQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f45cae5-918f-490e-c2f9-0f86dc77cf54"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"codes/test_speed.py\", line 6, in <module>\n",
            "    from codes.models import create_model\n",
            "  File \"/content/LPTN/codes/models/__init__.py\", line 17, in <module>\n",
            "    for file_name in model_filenames\n",
            "  File \"/content/LPTN/codes/models/__init__.py\", line 17, in <listcomp>\n",
            "    for file_name in model_filenames\n",
            "  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/content/LPTN/codes/models/lptn_model.py\", line 16, in <module>\n",
            "    metric_module = importlib.import_module('codes.metrics')\n",
            "  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/content/LPTN/codes/metrics/__init__.py\", line 1, in <module>\n",
            "    from .psnr_ssim import calculate_psnr, calculate_ssim\n",
            "  File \"/content/LPTN/codes/metrics/psnr_ssim.py\", line 4, in <module>\n",
            "    from basicsr.metrics.metric_util import reorder_image, to_y_channel\n",
            "ModuleNotFoundError: No module named 'basicsr'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training of model"
      ],
      "metadata": {
        "id": "tgM-ADEuxwBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q basicsr\n",
        "!PYTHONPATH=\"/content/LPTN/:${PYTHONPATH}\" CUDA_VISIBLE_DEVICES=0 python codes/train.py -opt options/train/LPTN/train_FiveK.yml"
      ],
      "metadata": {
        "id": "a2iCtLU1xtp0",
        "outputId": "60acabaa-bf26-499a-ddd5-5e34a3e5df44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting basicsr\n",
            "  Downloading basicsr-1.3.5.tar.gz (161 kB)\n",
            "\u001b[?25l\r\u001b[K     |██                              | 10 kB 26.9 MB/s eta 0:00:01\r\u001b[K     |████                            | 20 kB 30.9 MB/s eta 0:00:01\r\u001b[K     |██████                          | 30 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 40 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 51 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 61 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 71 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 81 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 92 kB 13.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 102 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 112 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 122 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 133 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 143 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 153 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 161 kB 12.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: addict in /usr/local/lib/python3.7/dist-packages (from basicsr) (2.4.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from basicsr) (0.16.0)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from basicsr) (0.99)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from basicsr) (1.21.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from basicsr) (4.1.2.30)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from basicsr) (7.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from basicsr) (3.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from basicsr) (2.23.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from basicsr) (0.18.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from basicsr) (1.4.1)\n",
            "Requirement already satisfied: tb-nightly in /usr/local/lib/python3.7/dist-packages (from basicsr) (2.9.0a20220313)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.7/dist-packages (from basicsr) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from basicsr) (0.11.1+cu111)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from basicsr) (4.63.0)\n",
            "Requirement already satisfied: yapf in /usr/local/lib/python3.7/dist-packages (from basicsr) (0.32.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7->basicsr) (3.10.0.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->basicsr) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->basicsr) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->basicsr) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->basicsr) (2021.10.8)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->basicsr) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->basicsr) (1.2.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->basicsr) (2.6.3)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->basicsr) (3.2.2)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->basicsr) (2021.11.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->basicsr) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->basicsr) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->basicsr) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->basicsr) (1.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->basicsr) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr) (1.44.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr) (0.6.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr) (3.17.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr) (1.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr) (0.4.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr) (0.37.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr) (57.4.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr) (1.35.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->basicsr) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->basicsr) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->basicsr) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly->basicsr) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tb-nightly->basicsr) (4.11.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tb-nightly->basicsr) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tb-nightly->basicsr) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly->basicsr) (3.2.0)\n",
            "Building wheels for collected packages: basicsr\n",
            "  Building wheel for basicsr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for basicsr: filename=basicsr-1.3.5-py3-none-any.whl size=194484 sha256=8ab53abde59a0954c14b0996750b866476629d3f599a0c4d7563ac30bdc1b015\n",
            "  Stored in directory: /root/.cache/pip/wheels/74/1b/d0/8659cf028233dd1e3bf282271009fbf037dfc4ab761f32a032\n",
            "Successfully built basicsr\n",
            "Installing collected packages: basicsr\n",
            "Successfully installed basicsr-1.3.5\n",
            "Disable distributed.\n",
            "2022-03-15 08:53:10,790 INFO: \n",
            "  name: LPTN_FiveK\n",
            "  model_type: LPTNModel\n",
            "  num_gpu: 2\n",
            "  manual_seed: 10\n",
            "  datasets:[\n",
            "    train:[\n",
            "      name: FiveK\n",
            "      type: UnPairedImageDataset\n",
            "      dataroot_gt: datasets/FiveK/FiveK_train_target.lmdb\n",
            "      dataroot_lq: datasets/FiveK/FiveK_train_source.lmdb\n",
            "      filename_tmpl: {}\n",
            "      io_backend:[\n",
            "        type: lmdb\n",
            "      ]\n",
            "      if_fix_size: True\n",
            "      gt_size: 256\n",
            "      use_flip: True\n",
            "      use_rot: True\n",
            "      use_shuffle: True\n",
            "      num_worker_per_gpu: 16\n",
            "      batch_size_per_gpu: 32\n",
            "      dataset_enlarge_ratio: 100\n",
            "      prefetch_mode: cuda\n",
            "      pin_memory: True\n",
            "      phase: train\n",
            "    ]\n",
            "    val:[\n",
            "      name: FiveK_val\n",
            "      type: PairedImageDataset\n",
            "      dataroot_gt: datasets/FiveK/FiveK_test_target.lmdb\n",
            "      dataroot_lq: datasets/FiveK/FiveK_test_source.lmdb\n",
            "      io_backend:[\n",
            "        type: lmdb\n",
            "      ]\n",
            "      phase: val\n",
            "    ]\n",
            "  ]\n",
            "  network_g:[\n",
            "    type: LPTN\n",
            "    nrb_low: 5\n",
            "    nrb_high: 3\n",
            "    num_high: 3\n",
            "  ]\n",
            "  network_d:[\n",
            "    type: Discriminator\n",
            "  ]\n",
            "  path:[\n",
            "    pretrain_network_g: None\n",
            "    strict_load_g: False\n",
            "    resume_state: None\n",
            "    root: /content/LPTN\n",
            "    experiments_root: /content/LPTN/experiments/LPTN_FiveK\n",
            "    models: /content/LPTN/experiments/LPTN_FiveK/models\n",
            "    training_states: /content/LPTN/experiments/LPTN_FiveK/training_states\n",
            "    log: /content/LPTN/experiments/LPTN_FiveK\n",
            "    visualization: /content/LPTN/experiments/LPTN_FiveK/visualization\n",
            "  ]\n",
            "  train:[\n",
            "    optim_g:[\n",
            "      type: Adam\n",
            "      lr: 0.0001\n",
            "      weight_decay: 0\n",
            "      betas: [0.9, 0.99]\n",
            "    ]\n",
            "    optim_d:[\n",
            "      type: Adam\n",
            "      lr: 0.0001\n",
            "      weight_decay: 0\n",
            "      betas: [0.9, 0.99]\n",
            "    ]\n",
            "    scheduler:[\n",
            "      type: MultiStepLR\n",
            "      milestones: [50000, 100000, 200000, 300000]\n",
            "      gamma: 0.5\n",
            "    ]\n",
            "    total_iter: 300000\n",
            "    warmup_iter: -1\n",
            "    pixel_opt:[\n",
            "      type: MSELoss\n",
            "      loss_weight: 1000\n",
            "      reduction: mean\n",
            "    ]\n",
            "    gan_opt:[\n",
            "      type: GANLoss\n",
            "      gan_type: standard\n",
            "      real_label_val: 1.0\n",
            "      fake_label_val: 0.0\n",
            "      loss_weight: 1\n",
            "    ]\n",
            "    gp_opt:[\n",
            "      loss_weight: 100\n",
            "    ]\n",
            "    net_d_iters: 1\n",
            "    net_d_init_iters: 0\n",
            "  ]\n",
            "  val:[\n",
            "    val_freq: 20000.0\n",
            "    save_img: True\n",
            "    metrics:[\n",
            "      psnr:[\n",
            "        type: calculate_psnr\n",
            "        crop_border: 4\n",
            "        test_y_channel: False\n",
            "      ]\n",
            "    ]\n",
            "  ]\n",
            "  logger:[\n",
            "    print_freq: 100\n",
            "    save_checkpoint_freq: 5000.0\n",
            "    use_tb_logger: True\n",
            "    wandb:[\n",
            "      project: None\n",
            "      resume_id: None\n",
            "    ]\n",
            "  ]\n",
            "  dist_params:[\n",
            "    backend: nccl\n",
            "    port: 29500\n",
            "  ]\n",
            "  is_train: True\n",
            "  dist: False\n",
            "  rank: 0\n",
            "  world_size: 1\n",
            "\n",
            "2022-03-15 08:53:13,567 INFO: Dataset UnPairedImageDataset - FiveK is created.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "2022-03-15 08:53:13,568 INFO: Training statistics:\n",
            "\tNumber of train images: 2250\n",
            "\tDataset enlarge ratio: 100\n",
            "\tBatch size per gpu: 32\n",
            "\tWorld size (gpu number): 1\n",
            "\tRequire iter number per epoch: 7032\n",
            "\tTotal epochs: 43; iters: 300000.\n",
            "2022-03-15 08:53:13,569 INFO: Dataset PairedImageDataset - FiveK_val is created.\n",
            "2022-03-15 08:53:13,569 INFO: Number of val images/folders in FiveK_val: 500\n",
            "2022-03-15 08:53:24,843 INFO: Network: DataParallel - LPTN, with parameters: 617,567\n",
            "2022-03-15 08:53:24,843 INFO: LPTN(\n",
            "  (lap_pyramid): Lap_Pyramid_Conv()\n",
            "  (trans_low): Trans_low(\n",
            "    (model): Sequential(\n",
            "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      (2): LeakyReLU(negative_slope=0.01)\n",
            "      (3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): LeakyReLU(negative_slope=0.01)\n",
            "      (5): ResidualBlock(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): LeakyReLU(negative_slope=0.01)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (6): ResidualBlock(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): LeakyReLU(negative_slope=0.01)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (7): ResidualBlock(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): LeakyReLU(negative_slope=0.01)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (8): ResidualBlock(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): LeakyReLU(negative_slope=0.01)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (9): ResidualBlock(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): LeakyReLU(negative_slope=0.01)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (10): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (11): LeakyReLU(negative_slope=0.01)\n",
            "      (12): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (trans_high): Trans_high(\n",
            "    (model): Sequential(\n",
            "      (0): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): LeakyReLU(negative_slope=0.01)\n",
            "      (2): ResidualBlock(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): LeakyReLU(negative_slope=0.01)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (3): ResidualBlock(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): LeakyReLU(negative_slope=0.01)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (4): ResidualBlock(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): LeakyReLU(negative_slope=0.01)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (5): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    )\n",
            "    (trans_mask_block_0): Sequential(\n",
            "      (0): Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): LeakyReLU(negative_slope=0.01)\n",
            "      (2): Conv2d(16, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (trans_mask_block_1): Sequential(\n",
            "      (0): Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): LeakyReLU(negative_slope=0.01)\n",
            "      (2): Conv2d(16, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (trans_mask_block_2): Sequential(\n",
            "      (0): Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): LeakyReLU(negative_slope=0.01)\n",
            "      (2): Conv2d(16, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "2022-03-15 08:53:24,853 INFO: Network: DataParallel - Discriminator, with parameters: 253,249\n",
            "2022-03-15 08:53:24,853 INFO: Discriminator(\n",
            "  (model): Sequential(\n",
            "    (0): Upsample(size=(256, 256), mode=bilinear)\n",
            "    (1): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (2): LeakyReLU(negative_slope=0.2)\n",
            "    (3): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "    (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (5): LeakyReLU(negative_slope=0.2)\n",
            "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (7): LeakyReLU(negative_slope=0.2)\n",
            "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (9): LeakyReLU(negative_slope=0.2)\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (11): LeakyReLU(negative_slope=0.2)\n",
            "    (12): Conv2d(128, 1, kernel_size=(8, 8), stride=(1, 1))\n",
            "  )\n",
            ")\n",
            "2022-03-15 08:53:24,854 INFO: Model [LPTNModel] is created.\n",
            "2022-03-15 08:54:53,041 INFO: Use cuda prefetch dataloader\n",
            "2022-03-15 08:54:53,044 INFO: Start training from epoch: 0, iter: 0\n",
            "2022-03-15 08:59:36,036 INFO: [LPTN_..][epoch:  0, iter:     100, lr:(1.000e-04,)] [eta: 12 days, 18:07:03, time (data): 1.702 (0.001)] l_g_pix: 3.2317e+00 l_g_gan: 6.3252e-02 l_d_real: 4.0507e-02 out_d_real: -4.0507e-02 l_d_fake: -6.2975e-02 out_d_fake: -6.2975e-02 \n",
            "2022-03-15 09:02:30,405 INFO: [LPTN_..][epoch:  0, iter:     200, lr:(1.000e-04,)] [eta: 9 days, 10:01:04, time (data): 1.742 (0.001)] l_g_pix: 2.2935e+00 l_g_gan: 6.9438e-02 l_d_real: 1.2538e-02 out_d_real: -1.2538e-02 l_d_fake: -6.8213e-02 out_d_fake: -6.8213e-02 \n",
            "2022-03-15 09:05:22,975 INFO: [LPTN_..][epoch:  0, iter:     300, lr:(1.000e-04,)] [eta: 8 days, 6:36:24, time (data): 1.647 (0.002)] l_g_pix: 1.6774e+00 l_g_gan: 1.3390e-01 l_d_real: 3.7095e-02 out_d_real: -3.7095e-02 l_d_fake: -1.3093e-01 out_d_fake: -1.3093e-01 \n",
            "2022-03-15 09:08:16,070 INFO: [LPTN_..][epoch:  0, iter:     400, lr:(1.000e-04,)] [eta: 7 days, 16:57:12, time (data): 1.677 (0.010)] l_g_pix: 1.0528e+00 l_g_gan: 1.0596e-01 l_d_real: -2.8330e-02 out_d_real: 2.8330e-02 l_d_fake: -1.0378e-01 out_d_fake: -1.0378e-01 \n",
            "2022-03-15 09:11:08,349 INFO: [LPTN_..][epoch:  0, iter:     500, lr:(1.000e-04,)] [eta: 7 days, 8:35:38, time (data): 1.737 (0.011)] l_g_pix: 1.0349e+00 l_g_gan: 2.0023e-01 l_d_real: -9.5805e-02 out_d_real: 9.5805e-02 l_d_fake: -2.0057e-01 out_d_fake: -2.0057e-01 \n",
            "2022-03-15 09:14:02,394 INFO: [LPTN_..][epoch:  0, iter:     600, lr:(1.000e-04,)] [eta: 7 days, 3:14:48, time (data): 1.597 (0.001)] l_g_pix: 1.0804e+00 l_g_gan: -5.7043e-02 l_d_real: -4.9338e-01 out_d_real: 4.9338e-01 l_d_fake: 7.1127e-02 out_d_fake: 7.1127e-02 \n",
            "2022-03-15 09:16:57,347 INFO: [LPTN_..][epoch:  0, iter:     700, lr:(1.000e-04,)] [eta: 6 days, 23:31:06, time (data): 1.883 (0.001)] l_g_pix: 1.1404e+00 l_g_gan: -1.3582e+00 l_d_real: -1.7566e+00 out_d_real: 1.7566e+00 l_d_fake: 1.1839e+00 out_d_fake: 1.1839e+00 \n",
            "2022-03-15 09:19:53,032 INFO: [LPTN_..][epoch:  0, iter:     800, lr:(1.000e-04,)] [eta: 6 days, 20:47:04, time (data): 1.590 (0.015)] l_g_pix: 1.1802e+00 l_g_gan: -3.8738e+00 l_d_real: -4.4310e+00 out_d_real: 4.4310e+00 l_d_fake: 3.7414e+00 out_d_fake: 3.7414e+00 \n",
            "2022-03-15 09:22:52,082 INFO: [LPTN_..][epoch:  0, iter:     900, lr:(1.000e-04,)] [eta: 6 days, 18:57:21, time (data): 1.599 (0.001)] l_g_pix: 1.1674e+00 l_g_gan: -4.3105e+00 l_d_real: -4.9751e+00 out_d_real: 4.9751e+00 l_d_fake: 4.2426e+00 out_d_fake: 4.2426e+00 \n",
            "2022-03-15 09:25:48,990 INFO: [LPTN_..][epoch:  0, iter:   1,000, lr:(1.000e-04,)] [eta: 6 days, 17:18:26, time (data): 1.868 (0.001)] l_g_pix: 8.7705e-01 l_g_gan: -5.5157e+00 l_d_real: -6.0508e+00 out_d_real: 6.0508e+00 l_d_fake: 5.8058e+00 out_d_fake: 5.8058e+00 \n",
            "2022-03-15 09:28:47,047 INFO: [LPTN_..][epoch:  0, iter:   1,100, lr:(1.000e-04,)] [eta: 6 days, 16:02:02, time (data): 1.643 (0.001)] l_g_pix: 1.0496e+00 l_g_gan: -6.8352e+00 l_d_real: -8.1677e+00 out_d_real: 8.1677e+00 l_d_fake: 6.8809e+00 out_d_fake: 6.8809e+00 \n",
            "2022-03-15 09:31:44,937 INFO: [LPTN_..][epoch:  0, iter:   1,200, lr:(1.000e-04,)] [eta: 6 days, 14:57:09, time (data): 1.837 (0.024)] l_g_pix: 9.0555e-01 l_g_gan: -6.9104e+00 l_d_real: -7.9746e+00 out_d_real: 7.9746e+00 l_d_fake: 7.1332e+00 out_d_fake: 7.1332e+00 \n",
            "2022-03-15 09:34:41,855 INFO: [LPTN_..][epoch:  0, iter:   1,300, lr:(1.000e-04,)] [eta: 6 days, 13:58:10, time (data): 1.608 (0.019)] l_g_pix: 1.0800e+00 l_g_gan: -7.7250e+00 l_d_real: -8.5001e+00 out_d_real: 8.5001e+00 l_d_fake: 7.6907e+00 out_d_fake: 7.6907e+00 \n",
            "2022-03-15 09:37:39,352 INFO: [LPTN_..][epoch:  0, iter:   1,400, lr:(1.000e-04,)] [eta: 6 days, 13:09:13, time (data): 1.656 (0.030)] l_g_pix: 1.2139e+00 l_g_gan: -9.1466e+00 l_d_real: -1.0238e+01 out_d_real: 1.0238e+01 l_d_fake: 9.2551e+00 out_d_fake: 9.2551e+00 \n",
            "2022-03-15 09:40:34,309 INFO: [LPTN_..][epoch:  0, iter:   1,500, lr:(1.000e-04,)] [eta: 6 days, 12:17:50, time (data): 1.720 (0.010)] l_g_pix: 1.1309e+00 l_g_gan: -9.1829e+00 l_d_real: -1.0182e+01 out_d_real: 1.0182e+01 l_d_fake: 9.2666e+00 out_d_fake: 9.2666e+00 \n",
            "2022-03-15 09:43:31,960 INFO: [LPTN_..][epoch:  0, iter:   1,600, lr:(1.000e-04,)] [eta: 6 days, 11:41:05, time (data): 1.939 (0.001)] l_g_pix: 1.1873e+00 l_g_gan: -9.4266e+00 l_d_real: -1.0037e+01 out_d_real: 1.0037e+01 l_d_fake: 9.4045e+00 out_d_fake: 9.4045e+00 \n",
            "2022-03-15 09:46:31,224 INFO: [LPTN_..][epoch:  0, iter:   1,700, lr:(1.000e-04,)] [eta: 6 days, 11:12:58, time (data): 2.061 (0.016)] l_g_pix: 1.1365e+00 l_g_gan: -1.0005e+01 l_d_real: -1.1452e+01 out_d_real: 1.1452e+01 l_d_fake: 1.0102e+01 out_d_fake: 1.0102e+01 \n",
            "2022-03-15 09:49:31,604 INFO: [LPTN_..][epoch:  0, iter:   1,800, lr:(1.000e-04,)] [eta: 6 days, 10:50:40, time (data): 1.957 (0.001)] l_g_pix: 1.1901e+00 l_g_gan: -1.0132e+01 l_d_real: -1.1513e+01 out_d_real: 1.1513e+01 l_d_fake: 1.0085e+01 out_d_fake: 1.0085e+01 \n",
            "2022-03-15 09:52:27,775 INFO: [LPTN_..][epoch:  0, iter:   1,900, lr:(1.000e-04,)] [eta: 6 days, 10:19:26, time (data): 1.666 (0.026)] l_g_pix: 1.6316e+00 l_g_gan: -1.3328e+01 l_d_real: -1.4499e+01 out_d_real: 1.4499e+01 l_d_fake: 1.3226e+01 out_d_fake: 1.3226e+01 \n",
            "2022-03-15 09:55:28,460 INFO: [LPTN_..][epoch:  0, iter:   2,000, lr:(1.000e-04,)] [eta: 6 days, 10:02:12, time (data): 2.122 (0.020)] l_g_pix: 1.7051e+00 l_g_gan: -1.4401e+01 l_d_real: -1.5728e+01 out_d_real: 1.5728e+01 l_d_fake: 1.4281e+01 out_d_fake: 1.4281e+01 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TWv4Wzovxy06"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}